{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "import io"
      ],
      "metadata": {
        "id": "O6UYIxeF-HOq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "IVMkCDbS7K4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c481bed6-728a-4ca5-b9d6-09163bebc480"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IsKCdLx56qW-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2f955e-a708-48e7-cf35-e1a460ba280d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss', 'compile_metrics']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('/content/drive/MyDrive/FACE RECOGNIZATION SYSTEM_model.keras')\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Recompile with explicit metrics\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "# Check if metrics are now available\n",
        "print(model.metrics_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate"
      ],
      "metadata": {
        "id": "1FJHNqSWGKvP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "fe92a04c-1293-43a2-a674-737229289efe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method TensorFlowTrainer.evaluate of <Sequential name=sequential, built=True>>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>keras.src.backend.tensorflow.trainer.TensorFlowTrainer.evaluate</b><br/>def evaluate(x=None, y=None, batch_size=None, verbose=&#x27;auto&#x27;, sample_weight=None, steps=None, callbacks=None, return_dict=False, **kwargs)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/trainer.py</a>Returns the loss value &amp; metrics values for the model in test mode.\n",
              "\n",
              "Computation is done in batches (see the `batch_size` arg.)\n",
              "\n",
              "Args:\n",
              "    x: Input data. It can be:\n",
              "        - A NumPy array (or array-like), or a list of arrays\n",
              "        (in case the model has multiple inputs).\n",
              "        - A backend-native tensor, or a list of tensors\n",
              "        (in case the model has multiple inputs).\n",
              "        - A dict mapping input names to the corresponding array/tensors,\n",
              "        if the model has named inputs.\n",
              "        - A `keras.utils.PyDataset` returning `(inputs, targets)` or\n",
              "        `(inputs, targets, sample_weights)`.\n",
              "        - A `tf.data.Dataset` yielding `(inputs, targets)` or\n",
              "        `(inputs, targets, sample_weights)`.\n",
              "        - A `torch.utils.data.DataLoader` yielding `(inputs, targets)`\n",
              "        or `(inputs, targets, sample_weights)`.\n",
              "        - A Python generator function yielding `(inputs, targets)` or\n",
              "        `(inputs, targets, sample_weights)`.\n",
              "    y: Target data. Like the input data `x`, it can be either NumPy\n",
              "        array(s) or backend-native tensor(s). If `x` is a\n",
              "        `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
              "        `torch.utils.data.DataLoader` or a Python generator function,\n",
              "        `y` should not be specified since targets will be obtained from\n",
              "        `x`.\n",
              "    batch_size: Integer or `None`.\n",
              "        Number of samples per batch of computation.\n",
              "        If unspecified, `batch_size` will default to 32.\n",
              "        Do not specify the `batch_size` if your input data `x` is a\n",
              "        `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
              "        `torch.utils.data.DataLoader` or Python generator function\n",
              "        since they generate batches.\n",
              "    verbose: `&quot;auto&quot;`, 0, 1, or 2. Verbosity mode.\n",
              "        0 = silent, 1 = progress bar, 2 = single line.\n",
              "        `&quot;auto&quot;` becomes 1 for most cases.\n",
              "        Note that the progress bar is not\n",
              "        particularly useful when logged to a file, so `verbose=2` is\n",
              "        recommended when not running interactively\n",
              "        (e.g. in a production environment). Defaults to `&quot;auto&quot;`.\n",
              "    sample_weight: Optional NumPy array or tensor of weights for\n",
              "        the training samples, used for weighting the loss function\n",
              "        (during training only). You can either pass a flat (1D)\n",
              "        NumPy array or tensor with the same length as the input samples\n",
              "        (1:1 mapping between weights and samples), or in the case of\n",
              "        temporal data, you can pass a 2D NumPy array or tensor with\n",
              "        shape `(samples, sequence_length)` to apply a different weight\n",
              "        to every timestep of every sample.\n",
              "        This argument is not supported when `x` is a\n",
              "        `keras.utils.PyDataset`, `tf.data.Dataset`,\n",
              "        `torch.utils.data.DataLoader` or Python generator function.\n",
              "        Instead, provide `sample_weights` as the third element of `x`.\n",
              "        Note that sample weighting does not apply to metrics specified\n",
              "        via the `metrics` argument in `compile()`. To apply sample\n",
              "        weighting to your metrics, you can specify them via the\n",
              "        `weighted_metrics` in `compile()` instead.\n",
              "    steps: Integer or `None`.\n",
              "        Total number of steps (batches of samples) to draw before\n",
              "        declaring the evaluation round finished. If `steps` is `None`,\n",
              "        it will run until `x` is exhausted. In the case of an infinitely\n",
              "        repeating dataset, it will run indefinitely.\n",
              "    callbacks: List of `keras.callbacks.Callback` instances.\n",
              "        List of callbacks to apply during evaluation.\n",
              "    return_dict: If `True`, loss and metric results are returned as a\n",
              "        dict, with each key being the name of the metric.\n",
              "        If `False`, they are returned as a list.\n",
              "\n",
              "Returns:\n",
              "    Scalar test loss (if the model has a single output and no metrics)\n",
              "    or list of scalars (if the model has multiple outputs\n",
              "    and/or metrics). The attribute `model.metrics_names` will give you\n",
              "    the display labels for the scalar outputs.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 433);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()\n",
        "\n",
        "\n",
        "# Ensure to print the model input shape for verification\n",
        "print(\"Model input shape:\", model.input_shape)\n"
      ],
      "metadata": {
        "id": "vhyZwyZV7nqc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "0585c00b-27be-4e94-9f89-4054369b9581"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m36,992\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m115200\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m7,372,864\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,992</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">115200</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">7,372,864</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,410,947\u001b[0m (28.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,410,947</span> (28.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,410,947\u001b[0m (28.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,410,947</span> (28.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model input shape: (None, 128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Camera** **ON**"
      ],
      "metadata": {
        "id": "9-5X0wi_AH1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL.Image\n",
        "import io\n",
        "import pickle\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "# Load the Haar Cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Load the training data (X and y) and class labels\n",
        "pickle_in = open(\"/content/drive/MyDrive/data.pkl\", \"rb\")\n",
        "data_load = pickle.load(pickle_in)  # Images\n",
        "\n",
        "pickle_in = open(\"/content/drive/MyDrive/label_encoder.pkl\", \"rb\")\n",
        "label_encoder= pickle.load(pickle_in)  # Labels\n",
        "\n",
        "# Load class labels (names)\n",
        "with open('/content/drive/MyDrive/FACE RECOGNIZATION SYSTEM/categories.pkl', 'rb') as f:\n",
        "    class_labels = pickle.load(f)\n",
        "\n",
        "print(\"Loaded Class Labels:\", class_labels)\n",
        "print(\"Loaded Labels:\", class_labels)\n",
        "\n",
        "\n",
        "\n",
        "# Function to preprocess the face for the model\n",
        "def preprocess_face(face, target_shape):\n",
        "    face_resized = cv2.resize(face, (target_shape[1], target_shape[2]))\n",
        "    face_normalized = face_resized / 255.0\n",
        "    face_array = np.expand_dims(face_normalized, axis=0)\n",
        "    if len(model.input_shape) == 2:\n",
        "        face_array = face_array.reshape((1, -1))\n",
        "    return face_array\n",
        "\n",
        "# Function to handle predictions\n",
        "def predict_label(face_array):\n",
        "    predictions = model.predict(face_array, verbose=0)\n",
        "    confidence = np.max(predictions)\n",
        "    label_index = np.argmax(predictions)\n",
        "\n",
        "    return label_index, confidence\n",
        "\n",
        "\n",
        "# Function to convert JavaScript object to OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "    image_bytes = b64decode(js_reply.split(',')[1])\n",
        "    jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "    img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "    return img\n",
        "\n",
        "# Function to convert OpenCV bounding box to base64 for overlay\n",
        "def bbox_to_bytes(bbox_array):\n",
        "    # Ensure bbox_array is in RGBA format\n",
        "    if len(bbox_array.shape) == 3 and bbox_array.shape[2] != 4:\n",
        "        bbox_array = cv2.cvtColor(bbox_array, cv2.COLOR_BGR2BGRA)  # Convert to BGRA if not already\n",
        "\n",
        "    # Convert NumPy array to PIL Image\n",
        "    bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "\n",
        "    # Create an in-memory byte buffer\n",
        "    iobuf = io.BytesIO()\n",
        "\n",
        "    # Save the image as PNG to the byte buffer\n",
        "    bbox_PIL.save(iobuf, format='PNG')  # Use PNG to retain transparency\n",
        "\n",
        "    # Convert to base64 string and return\n",
        "    bbox_bytes = 'data:image/png;base64,{}'.format(str(b64encode(iobuf.getvalue()), 'utf-8'))\n",
        "    return bbox_bytes\n",
        "\n",
        "\n",
        "# JavaScript for video stream\n",
        "def video_stream():\n",
        "    js = Javascript('''\n",
        "        var video;\n",
        "        var div = null;\n",
        "        var stream;\n",
        "        var captureCanvas;\n",
        "        var imgElement;\n",
        "        var labelElement;\n",
        "\n",
        "        var pendingResolve = null;\n",
        "        var shutdown = false;\n",
        "\n",
        "        function removeDom() {\n",
        "            stream.getVideoTracks()[0].stop();\n",
        "            video.remove();\n",
        "            div.remove();\n",
        "            video = null;\n",
        "            div = null;\n",
        "            stream = null;\n",
        "            imgElement = null;\n",
        "            captureCanvas = null;\n",
        "            labelElement = null;\n",
        "        }\n",
        "\n",
        "        function onAnimationFrame() {\n",
        "            if (!shutdown) {\n",
        "                window.requestAnimationFrame(onAnimationFrame);\n",
        "            }\n",
        "            if (pendingResolve) {\n",
        "                var result = \"\";\n",
        "                if (!shutdown) {\n",
        "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "                    result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "                }\n",
        "                var lp = pendingResolve;\n",
        "                pendingResolve = null;\n",
        "                lp(result);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        async function createDom() {\n",
        "            if (div !== null) {\n",
        "                return stream;\n",
        "            }\n",
        "\n",
        "            div = document.createElement('div');\n",
        "            div.style.border = '2px solid black';\n",
        "            div.style.padding = '3px';\n",
        "            div.style.width = '100%';\n",
        "            div.style.height = '100%';\n",
        "            div.style.maxWidth = '600px';\n",
        "            document.body.appendChild(div);\n",
        "\n",
        "            const modelOut = document.createElement('div');\n",
        "            modelOut.innerHTML = \"Status:\";\n",
        "            labelElement = document.createElement('span');\n",
        "            labelElement.innerText = 'No data';\n",
        "            labelElement.style.fontWeight = 'bold';\n",
        "            modelOut.appendChild(labelElement);\n",
        "            div.appendChild(modelOut);\n",
        "\n",
        "            video = document.createElement('video');\n",
        "            video.style.display = 'block';\n",
        "            video.width = div.clientWidth - 6;\n",
        "            video.setAttribute('playsinline', '');\n",
        "            video.onclick = () => { shutdown = true; };\n",
        "            stream = await navigator.mediaDevices.getUserMedia(\n",
        "                {video: { facingMode: \"environment\"}});\n",
        "            div.appendChild(video);\n",
        "\n",
        "            imgElement = document.createElement('img');\n",
        "            imgElement.style.position = 'absolute';\n",
        "            imgElement.style.zIndex = 1;\n",
        "            imgElement.onclick = () => { shutdown = true; };\n",
        "            div.appendChild(imgElement);\n",
        "\n",
        "            const instruction = document.createElement('div');\n",
        "            instruction.innerHTML =\n",
        "                '' +\n",
        "                '';\n",
        "            div.appendChild(instruction);\n",
        "            instruction.onclick = () => { shutdown = true; };\n",
        "\n",
        "            // Add key press event listener\n",
        "            window.addEventListener('keydown', function(e) {\n",
        "                if (e.key === 's' || e.key === 'S') {\n",
        "                    shutdown = true;  // Stop the video stream on 'S' key press\n",
        "                }\n",
        "            });\n",
        "\n",
        "            video.srcObject = stream;\n",
        "            await video.play();\n",
        "\n",
        "            captureCanvas = document.createElement('canvas');\n",
        "            captureCanvas.width = 640;\n",
        "            captureCanvas.height = 480;\n",
        "            window.requestAnimationFrame(onAnimationFrame);\n",
        "\n",
        "            return stream;\n",
        "        }\n",
        "\n",
        "        async function stream_frame(label, imgData) {\n",
        "            if (shutdown) {\n",
        "                removeDom();\n",
        "                shutdown = false;\n",
        "                return '';\n",
        "            }\n",
        "\n",
        "            var preCreate = Date.now();\n",
        "            stream = await createDom();\n",
        "\n",
        "            var preShow = Date.now();\n",
        "            if (label != \"\") {\n",
        "                labelElement.innerHTML = label;\n",
        "            }\n",
        "\n",
        "            if (imgData != \"\") {\n",
        "                var videoRect = video.getClientRects()[0];\n",
        "                imgElement.style.top = videoRect.top + \"px\";\n",
        "                imgElement.style.left = videoRect.left + \"px\";\n",
        "                imgElement.style.width = videoRect.width + \"px\";\n",
        "                imgElement.style.height = videoRect.height + \"px\";\n",
        "                imgElement.src = imgData;\n",
        "            }\n",
        "\n",
        "            var preCapture = Date.now();\n",
        "            var result = await new Promise(function(resolve, reject) {\n",
        "                pendingResolve = resolve;\n",
        "            });\n",
        "            shutdown = false;\n",
        "\n",
        "            return {'create': preShow - preCreate,\n",
        "                    'show': preCapture - preShow,\n",
        "                    'capture': Date.now() - preCapture,\n",
        "                    'img': result};\n",
        "        }\n",
        "    ''')\n",
        "    display(js)\n",
        "\n",
        "def video_frame(label, bbox):\n",
        "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "    return data\n",
        "\n",
        "# Start video stream\n",
        "video_stream()\n",
        "label_html = 'Capturing...'\n",
        "bbox = ''\n",
        "count = 0\n",
        "\n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # Convert JS response to OpenCV image\n",
        "    img = js_to_image(js_reply[\"img\"])\n",
        "    bbox_array = np.zeros([480, 640, 3], dtype=np.uint8)  # Use 3 channels for BGR format\n",
        "\n",
        "    # Convert image to grayscale for face detection\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "    # Draw bounding box and label on the image\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract and preprocess the detected face\n",
        "        face = img[y:y+h, x:x+w]\n",
        "        face_array = preprocess_face(face, target_shape=model.input_shape)\n",
        "\n",
        "        # Predict the class and confidence\n",
        "        label_index, confidence = predict_label(face_array)\n",
        "\n",
        "        # Determine the label based on confidence\n",
        "        if confidence > 0.8:  # Lowered confidence threshold for debugging\n",
        "            label = f\"{class_labels[label_index]} ({confidence*100:.2f}%)\"\n",
        "            color = (0, 255, 0)  # Green for recognized\n",
        "        else:\n",
        "            label = \"Unknown\"\n",
        "            color = (0, 0, 255)  # Red for unknown\n",
        "\n",
        "        # Draw the bounding box and label on the image\n",
        "        cv2.rectangle(bbox_array, (x, y), (x+w, y+h), color, 2)\n",
        "        cv2.putText(bbox_array, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
        "\n",
        "\n",
        "    # Convert bbox_array to RGB format if not already\n",
        "    bbox_array = cv2.cvtColor(bbox_array, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Create an alpha channel for transparency\n",
        "    # Reduce the alpha value for partial transparency (e.g., 128 for 50% transparency)\n",
        "    alpha_channel = np.full(bbox_array.shape[:2], 128, dtype=np.uint8)\n",
        "\n",
        "    # Add the alpha channel to the bbox_array\n",
        "    bbox_array_with_alpha = np.dstack([bbox_array, alpha_channel])\n",
        "\n",
        "    # Convert the RGBA array to bytes\n",
        "    bbox_bytes = bbox_to_bytes(bbox_array_with_alpha)\n",
        "\n",
        "    # Send the overlay with the bounding box back to the frontend\n",
        "    bbox = bbox_bytes\n",
        "\n"
      ],
      "metadata": {
        "id": "LcXd3QtuPGcy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "outputId": "02e54b1f-84aa-46d5-b0e1-1e39620b3655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded Class Labels: {0: np.str_('AVISHEK'), 1: np.str_('LAMYEA'), 2: np.str_('SAGOR')}\n",
            "Loaded Labels: {0: np.str_('AVISHEK'), 1: np.str_('LAMYEA'), 2: np.str_('SAGOR')}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        var video;\n",
              "        var div = null;\n",
              "        var stream;\n",
              "        var captureCanvas;\n",
              "        var imgElement;\n",
              "        var labelElement;\n",
              "\n",
              "        var pendingResolve = null;\n",
              "        var shutdown = false;\n",
              "\n",
              "        function removeDom() {\n",
              "            stream.getVideoTracks()[0].stop();\n",
              "            video.remove();\n",
              "            div.remove();\n",
              "            video = null;\n",
              "            div = null;\n",
              "            stream = null;\n",
              "            imgElement = null;\n",
              "            captureCanvas = null;\n",
              "            labelElement = null;\n",
              "        }\n",
              "\n",
              "        function onAnimationFrame() {\n",
              "            if (!shutdown) {\n",
              "                window.requestAnimationFrame(onAnimationFrame);\n",
              "            }\n",
              "            if (pendingResolve) {\n",
              "                var result = \"\";\n",
              "                if (!shutdown) {\n",
              "                    captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "                    result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
              "                }\n",
              "                var lp = pendingResolve;\n",
              "                pendingResolve = null;\n",
              "                lp(result);\n",
              "            }\n",
              "        }\n",
              "\n",
              "        async function createDom() {\n",
              "            if (div !== null) {\n",
              "                return stream;\n",
              "            }\n",
              "\n",
              "            div = document.createElement('div');\n",
              "            div.style.border = '2px solid black';\n",
              "            div.style.padding = '3px';\n",
              "            div.style.width = '100%';\n",
              "            div.style.height = '100%';\n",
              "            div.style.maxWidth = '600px';\n",
              "            document.body.appendChild(div);\n",
              "\n",
              "            const modelOut = document.createElement('div');\n",
              "            modelOut.innerHTML = \"Status:\";\n",
              "            labelElement = document.createElement('span');\n",
              "            labelElement.innerText = 'No data';\n",
              "            labelElement.style.fontWeight = 'bold';\n",
              "            modelOut.appendChild(labelElement);\n",
              "            div.appendChild(modelOut);\n",
              "\n",
              "            video = document.createElement('video');\n",
              "            video.style.display = 'block';\n",
              "            video.width = div.clientWidth - 6;\n",
              "            video.setAttribute('playsinline', '');\n",
              "            video.onclick = () => { shutdown = true; };\n",
              "            stream = await navigator.mediaDevices.getUserMedia(\n",
              "                {video: { facingMode: \"environment\"}});\n",
              "            div.appendChild(video);\n",
              "\n",
              "            imgElement = document.createElement('img');\n",
              "            imgElement.style.position = 'absolute';\n",
              "            imgElement.style.zIndex = 1;\n",
              "            imgElement.onclick = () => { shutdown = true; };\n",
              "            div.appendChild(imgElement);\n",
              "\n",
              "            const instruction = document.createElement('div');\n",
              "            instruction.innerHTML =\n",
              "                '' +\n",
              "                '';\n",
              "            div.appendChild(instruction);\n",
              "            instruction.onclick = () => { shutdown = true; };\n",
              "\n",
              "            // Add key press event listener\n",
              "            window.addEventListener('keydown', function(e) {\n",
              "                if (e.key === 's' || e.key === 'S') {\n",
              "                    shutdown = true;  // Stop the video stream on 'S' key press\n",
              "                }\n",
              "            });\n",
              "\n",
              "            video.srcObject = stream;\n",
              "            await video.play();\n",
              "\n",
              "            captureCanvas = document.createElement('canvas');\n",
              "            captureCanvas.width = 640;\n",
              "            captureCanvas.height = 480;\n",
              "            window.requestAnimationFrame(onAnimationFrame);\n",
              "\n",
              "            return stream;\n",
              "        }\n",
              "\n",
              "        async function stream_frame(label, imgData) {\n",
              "            if (shutdown) {\n",
              "                removeDom();\n",
              "                shutdown = false;\n",
              "                return '';\n",
              "            }\n",
              "\n",
              "            var preCreate = Date.now();\n",
              "            stream = await createDom();\n",
              "\n",
              "            var preShow = Date.now();\n",
              "            if (label != \"\") {\n",
              "                labelElement.innerHTML = label;\n",
              "            }\n",
              "\n",
              "            if (imgData != \"\") {\n",
              "                var videoRect = video.getClientRects()[0];\n",
              "                imgElement.style.top = videoRect.top + \"px\";\n",
              "                imgElement.style.left = videoRect.left + \"px\";\n",
              "                imgElement.style.width = videoRect.width + \"px\";\n",
              "                imgElement.style.height = videoRect.height + \"px\";\n",
              "                imgElement.src = imgData;\n",
              "            }\n",
              "\n",
              "            var preCapture = Date.now();\n",
              "            var result = await new Promise(function(resolve, reject) {\n",
              "                pendingResolve = resolve;\n",
              "            });\n",
              "            shutdown = false;\n",
              "\n",
              "            return {'create': preShow - preCreate,\n",
              "                    'show': preCapture - preShow,\n",
              "                    'capture': Date.now() - preCapture,\n",
              "                    'img': result};\n",
              "        }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1622287508.py:60: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
            "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n"
          ]
        }
      ]
    }
  ]
}